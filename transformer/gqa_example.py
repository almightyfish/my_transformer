"""
åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (Grouped Query Attention, GQA) ç¤ºä¾‹

GQAæ˜¯ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»‹äºå¤šå¤´æ³¨æ„åŠ›(MHA)å’Œå¤šæŸ¥è¯¢æ³¨æ„åŠ›(MQA)ä¹‹é—´ã€‚
é€šè¿‡å‡å°‘key-valueå¤´çš„æ•°é‡æ¥é™ä½å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚

ä¸»è¦ä¼˜åŠ¿ï¼š
1. å‡å°‘å†…å­˜å ç”¨ - keyå’Œvalueä½¿ç”¨æ›´å°‘çš„å¤´æ•°
2. æé«˜è®¡ç®—æ•ˆç‡ - å‡å°‘KV-cacheçš„å¤§å°
3. ä¿æŒæ€§èƒ½ - æ¯”MQAæ€§èƒ½æ›´å¥½ï¼Œæ¯”MHAæ•ˆç‡æ›´é«˜
4. çµæ´»é…ç½® - å¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´KVå¤´æ•°

é€‚ç”¨åœºæ™¯ï¼š
- é•¿åºåˆ—å¤„ç†
- èµ„æºå—é™ç¯å¢ƒ
- æ¨ç†ä¼˜åŒ–
- å¤§å‹è¯­è¨€æ¨¡å‹
"""

import torch
import torch.nn as nn
from .attention import (
    MultiHeadAttention, 
    GroupedQueryAttention, 
    GQASelfAttention,
    RoPEGroupedQueryAttention,
    RoGQASelfAttention
)


def compare_attention_mechanisms():
    """æ¯”è¾ƒä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡"""
    
    print("=== æ³¨æ„åŠ›æœºåˆ¶æ¯”è¾ƒ ===\n")
    
    # é…ç½®å‚æ•°
    batch_size = 2
    seq_len = 128
    d_model = 512
    num_heads = 8
    num_kv_heads = 2  # GQAä½¿ç”¨æ›´å°‘çš„KVå¤´
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, d_model)
    
    # 1. å¤šå¤´æ³¨æ„åŠ› (MHA)
    print("1. å¤šå¤´æ³¨æ„åŠ› (MHA)")
    mha = MultiHeadAttention(d_model, num_heads)
    mha_params = sum(p.numel() for p in mha.parameters())
    print(f"   å‚æ•°é‡: {mha_params:,}")
    
    with torch.no_grad():
        mha_output, mha_weights = mha(x, x, x)
        print(f"   è¾“å‡ºå½¢çŠ¶: {mha_output.shape}")
        print(f"   æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {mha_weights.shape}")
    
    # 2. åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA)
    print("\n2. åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA)")
    gqa = GroupedQueryAttention(d_model, num_heads, num_kv_heads)
    gqa_params = sum(p.numel() for p in gqa.parameters())
    print(f"   å‚æ•°é‡: {gqa_params:,}")
    print(f"   å‚æ•°å‡å°‘: {((mha_params - gqa_params) / mha_params * 100):.1f}%")
    print(f"   æŸ¥è¯¢å¤´æ•°: {num_heads}, KVå¤´æ•°: {num_kv_heads}")
    print(f"   æ¯ä¸ªKVå¤´å¯¹åº”: {num_heads // num_kv_heads} ä¸ªæŸ¥è¯¢å¤´")
    
    with torch.no_grad():
        gqa_output, gqa_weights = gqa(x, x, x)
        print(f"   è¾“å‡ºå½¢çŠ¶: {gqa_output.shape}")
        print(f"   æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {gqa_weights.shape}")
    
    # 3. GQAè‡ªæ³¨æ„åŠ›
    print("\n3. GQAè‡ªæ³¨æ„åŠ›")
    gqa_self = GQASelfAttention(d_model, num_heads, num_kv_heads)
    
    with torch.no_grad():
        gqa_self_output, _ = gqa_self(x)
        print(f"   è¾“å‡ºå½¢çŠ¶: {gqa_self_output.shape}")
    
    print()


def demonstrate_rope_gqa():
    """æ¼”ç¤ºæ”¯æŒRoPEçš„GQA"""
    
    print("=== RoPE + GQA ç»„åˆç¤ºä¾‹ ===\n")
    
    # é…ç½®å‚æ•°
    batch_size = 2
    seq_len = 64
    d_model = 256
    num_heads = 8
    num_kv_heads = 4
    max_seq_len = 512
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, d_model)
    
    # 1. RoPE + GQA
    print("1. RoPEåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›")
    rope_gqa = RoPEGroupedQueryAttention(
        d_model=d_model,
        num_heads=num_heads,
        num_kv_heads=num_kv_heads,
        max_seq_len=max_seq_len,
        rope_base=10000
    )
    
    with torch.no_grad():
        output, weights = rope_gqa(x, x, x)
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   å…·æœ‰ä½ç½®ç¼–ç çš„æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {weights.shape}")
    
    # 2. RoPE + GQA è‡ªæ³¨æ„åŠ›
    print("\n2. RoPE GQAè‡ªæ³¨æ„åŠ›")
    rope_gqa_self = RoGQASelfAttention(
        d_model=d_model,
        num_heads=num_heads,
        num_kv_heads=num_kv_heads,
        max_seq_len=max_seq_len
    )
    
    with torch.no_grad():
        self_output, _ = rope_gqa_self(x)
        print(f"   è‡ªæ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {self_output.shape}")
    
    print()


def efficiency_analysis():
    """åˆ†æä¸åŒé…ç½®çš„æ•ˆç‡"""
    
    print("=== GQA æ•ˆç‡åˆ†æ ===\n")
    
    d_model = 768
    seq_len = 256
    batch_size = 4
    
    configurations = [
        {"name": "æ ‡å‡†MHA", "num_heads": 12, "num_kv_heads": 12},
        {"name": "GQA-4", "num_heads": 12, "num_kv_heads": 4},
        {"name": "GQA-2", "num_heads": 12, "num_kv_heads": 2},
        {"name": "MQA", "num_heads": 12, "num_kv_heads": 1},
    ]
    
    x = torch.randn(batch_size, seq_len, d_model)
    
    print(f"{'é…ç½®':<10} {'å‚æ•°é‡':<15} {'å‚æ•°æ¯”ä¾‹':<10} {'KVç¼“å­˜æ¯”ä¾‹':<12} {'æè¿°'}")
    print("-" * 70)
    
    base_params = None
    
    for config in configurations:
        if config["num_kv_heads"] == config["num_heads"]:
            # æ ‡å‡†MHA
            model = MultiHeadAttention(d_model, config["num_heads"])
        else:
            # GQAæˆ–MQA
            model = GroupedQueryAttention(d_model, config["num_heads"], config["num_kv_heads"])
        
        params = sum(p.numel() for p in model.parameters())
        
        if base_params is None:
            base_params = params
            param_ratio = "100%"
        else:
            param_ratio = f"{params/base_params*100:.0f}%"
        
        # KVç¼“å­˜å¤§å°æ¯”ä¾‹ï¼ˆä¸»è¦æ˜¯Kå’ŒVçš„å‚æ•°ï¼‰
        kv_ratio = f"{config['num_kv_heads']/12*100:.0f}%"
        
        # æè¿°
        if config["num_kv_heads"] == config["num_heads"]:
            desc = "æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›"
        elif config["num_kv_heads"] == 1:
            desc = "å¤šæŸ¥è¯¢æ³¨æ„åŠ›"
        else:
            desc = f"æ¯{config['num_heads']//config['num_kv_heads']}ä¸ªQå…±äº«1ä¸ªKV"
        
        print(f"{config['name']:<10} {params:<15,} {param_ratio:<10} {kv_ratio:<12} {desc}")
    
    print()


def practical_usage_example():
    """å®é™…ä½¿ç”¨åœºæ™¯ç¤ºä¾‹"""
    
    print("=== å®é™…ä½¿ç”¨åœºæ™¯ ===\n")
    
    # åœºæ™¯1ï¼šé•¿åºåˆ—å¤„ç†
    print("åœºæ™¯1: é•¿åºåˆ—æ–‡æ¡£å¤„ç†")
    print("- åºåˆ—é•¿åº¦: 2048")
    print("- æ¨¡å‹ç»´åº¦: 768") 
    print("- ä½¿ç”¨GQA-4æ¥å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡")
    
    long_seq_model = GQASelfAttention(
        d_model=768,
        num_heads=12,
        num_kv_heads=4,  # 1/3çš„KVå¤´æ•°
        dropout=0.1
    )
    
    # æ¨¡æ‹Ÿé•¿åºåˆ—
    long_x = torch.randn(1, 2048, 768)
    with torch.no_grad():
        long_output, _ = long_seq_model(long_x)
        print(f"   å¤„ç†ç»“æœå½¢çŠ¶: {long_output.shape}")
    
    print()
    
    # åœºæ™¯2ï¼šç§»åŠ¨è®¾å¤‡éƒ¨ç½²
    print("åœºæ™¯2: ç§»åŠ¨è®¾å¤‡/è¾¹ç¼˜è®¡ç®—")
    print("- æ¨¡å‹ç»´åº¦: 384")
    print("- ä½¿ç”¨GQA-1(MQA)ä»¥æœ€å¤§åŒ–æ•ˆç‡")
    
    mobile_model = RoGQASelfAttention(
        d_model=384,
        num_heads=6,
        num_kv_heads=1,  # æè‡´çš„å‚æ•°å‹ç¼©
        max_seq_len=512
    )
    
    mobile_params = sum(p.numel() for p in mobile_model.parameters())
    print(f"   æ¨¡å‹å‚æ•°é‡: {mobile_params:,}")
    
    # æ¨¡æ‹Ÿç§»åŠ¨è®¾å¤‡è¾“å…¥
    mobile_x = torch.randn(1, 128, 384)
    with torch.no_grad():
        mobile_output, _ = mobile_model(mobile_x)
        print(f"   å¤„ç†ç»“æœå½¢çŠ¶: {mobile_output.shape}")
    
    print()


def performance_optimization_tips():
    """æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    
    print("=== GQA æ€§èƒ½ä¼˜åŒ–å»ºè®® ===\n")
    
    tips = [
        "1. KVå¤´æ•°é€‰æ‹©ï¼š",
        "   - é€šå¸¸é€‰æ‹©æŸ¥è¯¢å¤´æ•°çš„1/2åˆ°1/4",
        "   - ä¾‹å¦‚ï¼š12ä¸ªæŸ¥è¯¢å¤´ -> 2-4ä¸ªKVå¤´",
        "   - ä¿è¯num_headsèƒ½è¢«num_kv_headsæ•´é™¤",
        "",
        "2. å†…å­˜ä¼˜åŒ–ï¼š",
        "   - GQA-4ç›¸æ¯”MHAå‡å°‘çº¦33%çš„KVç¼“å­˜",
        "   - MQAç›¸æ¯”MHAå‡å°‘çº¦92%çš„KVç¼“å­˜",
        "   - é€‚åˆé•¿åºåˆ—å’Œæ‰¹å¤„ç†",
        "",
        "3. è®¡ç®—æ•ˆç‡ï¼š",
        "   - è®­ç»ƒæ—¶æ•ˆæœæ˜æ˜¾ï¼Œæ¨ç†æ—¶æ•ˆæœæ›´ä½³",
        "   - ä¸RoPEç»“åˆä½¿ç”¨æ•ˆæœæ›´å¥½",
        "   - é€‚åˆTransformerå¤§æ¨¡å‹",
        "",
        "4. è´¨é‡å¹³è¡¡ï¼š",
        "   - GQA-4: 95-98%çš„MHAæ€§èƒ½",
        "   - GQA-2: 90-95%çš„MHAæ€§èƒ½", 
        "   - MQA: 85-90%çš„MHAæ€§èƒ½",
        "",
        "5. ä½¿ç”¨åœºæ™¯ï¼š",
        "   - æ¨ç†æœåŠ¡ï¼šä¼˜å…ˆé€‰æ‹©GQAæˆ–MQA",
        "   - è®­ç»ƒé˜¶æ®µï¼šå¯ä»¥ä½¿ç”¨GQAåŠ é€Ÿ",
        "   - èµ„æºå—é™ï¼šä½¿ç”¨MQA",
        "   - é«˜è´¨é‡è¦æ±‚ï¼šä½¿ç”¨GQA-4"
    ]
    
    for tip in tips:
        print(tip)
    
    print()


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    
    print("ğŸš€ åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA) å®Œæ•´ç¤ºä¾‹\n")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯é‡ç°
    torch.manual_seed(42)
    
    # è¿è¡Œå„ä¸ªç¤ºä¾‹
    compare_attention_mechanisms()
    demonstrate_rope_gqa()
    efficiency_analysis()
    practical_usage_example()
    performance_optimization_tips()
    
    print("=" * 50)
    print("âœ… æ‰€æœ‰GQAç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("\nğŸ’¡ æç¤ºï¼š")
    print("- GQAæ˜¯ç°ä»£Transformerçš„é‡è¦ä¼˜åŒ–æŠ€æœ¯")
    print("- å»ºè®®åœ¨å®é™…é¡¹ç›®ä¸­æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„KVå¤´æ•°")
    print("- å¯ä»¥ä¸RoPEç­‰å…¶ä»–æŠ€æœ¯ç»„åˆä½¿ç”¨ä»¥è·å¾—æ›´å¥½æ•ˆæœ")


if __name__ == "__main__":
    main() 