#!/usr/bin/env python3
"""
RoPE (Rotary Position Embedding) ä½¿ç”¨ç¤ºä¾‹

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨RoPEä½ç½®ç¼–ç ï¼š
1. åŸºæœ¬çš„RoPEä½¿ç”¨
2. æ”¯æŒRoPEçš„å¤šå¤´æ³¨æ„åŠ›
3. RoPE vs ä¼ ç»Ÿä½ç½®ç¼–ç çš„æ¯”è¾ƒ
4. åœ¨Transformerä¸­é›†æˆRoPE
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

# å¦‚æœåœ¨åŒ…å†…è¿è¡Œï¼Œä½¿ç”¨ç›¸å¯¹å¯¼å…¥
try:
    from .positional_encoding import RoPE, PositionalEncoding
    from .attention import RoPEMultiHeadAttention, MultiHeadAttention
    from .utils import create_padding_mask
except ImportError:
    # å¦‚æœç›´æ¥è¿è¡Œè¯¥æ–‡ä»¶ï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥
    from transformer.positional_encoding import RoPE, PositionalEncoding
    from transformer.attention import RoPEMultiHeadAttention, MultiHeadAttention
    from transformer.utils import create_padding_mask


def basic_rope_example():
    """åŸºæœ¬RoPEä½¿ç”¨ç¤ºä¾‹"""
    print("=" * 50)
    print("åŸºæœ¬RoPEä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # è®¾ç½®å‚æ•°
    d_model = 128
    seq_len = 10
    batch_size = 2
    num_heads = 8
    head_dim = d_model // num_heads
    
    # åˆ›å»ºRoPEå®ä¾‹
    rope = RoPE(head_dim, max_seq_len=100)
    print(f"RoPEåˆ›å»ºå®Œæˆ, head_dim={head_dim}")
    
    # åˆ›å»ºç¤ºä¾‹å¼ é‡
    q = torch.randn(batch_size, num_heads, seq_len, head_dim)
    k = torch.randn(batch_size, num_heads, seq_len, head_dim)
    
    print(f"åŸå§‹æŸ¥è¯¢å¼ é‡å½¢çŠ¶: {q.shape}")
    print(f"åŸå§‹é”®å¼ é‡å½¢çŠ¶: {k.shape}")
    
    # åº”ç”¨RoPE
    q_rope, k_rope = rope(q, k)
    
    print(f"åº”ç”¨RoPEåæŸ¥è¯¢å¼ é‡å½¢çŠ¶: {q_rope.shape}")
    print(f"åº”ç”¨RoPEåé”®å¼ é‡å½¢çŠ¶: {k_rope.shape}")
    
    # éªŒè¯RoPEçš„æ—‹è½¬ä¸å˜æ€§
    # å¯¹äºç›¸å¯¹ä½ç½®ä¸º0çš„æƒ…å†µï¼Œå†…ç§¯åº”è¯¥ä¿æŒä¸å˜
    original_self_attn = torch.sum(q * k, dim=-1)  # [batch, heads, seq_len]
    rope_self_attn = torch.sum(q_rope * k_rope, dim=-1)
    
    print(f"åŸå§‹è‡ªæ³¨æ„åŠ›åˆ†æ•° (å‰3ä¸ªä½ç½®): {original_self_attn[0, 0, :3]}")
    print(f"RoPEè‡ªæ³¨æ„åŠ›åˆ†æ•° (å‰3ä¸ªä½ç½®): {rope_self_attn[0, 0, :3]}")
    print(f"å·®å¼‚: {torch.mean(torch.abs(original_self_attn - rope_self_attn)):.6f}")


def rope_attention_comparison():
    """RoPEæ³¨æ„åŠ› vs ä¼ ç»Ÿæ³¨æ„åŠ›æ¯”è¾ƒ"""
    print("\n" + "=" * 50)
    print("RoPEæ³¨æ„åŠ› vs ä¼ ç»Ÿæ³¨æ„åŠ›æ¯”è¾ƒ")
    print("=" * 50)
    
    # å‚æ•°è®¾ç½®
    d_model = 256
    num_heads = 8
    seq_len = 32
    batch_size = 4
    
    # åˆ›å»ºè¾“å…¥æ•°æ®
    x = torch.randn(batch_size, seq_len, d_model)
    mask = create_padding_mask(torch.ones(batch_size, seq_len), pad_idx=0)
    
    # ä¼ ç»Ÿå¤šå¤´æ³¨æ„åŠ›
    traditional_attn = MultiHeadAttention(d_model, num_heads)
    rope_attn = RoPEMultiHeadAttention(d_model, num_heads)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        traditional_output, traditional_weights = traditional_attn(x, x, x, mask)
        rope_output, rope_weights = rope_attn(x, x, x, mask)
    
    print(f"ä¼ ç»Ÿæ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {traditional_output.shape}")
    print(f"RoPEæ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {rope_output.shape}")
    
    # æ¯”è¾ƒæ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
    print(f"ä¼ ç»Ÿæ³¨æ„åŠ›æƒé‡å¹³å‡å€¼: {traditional_weights.mean():.6f}")
    print(f"RoPEæ³¨æ„åŠ›æƒé‡å¹³å‡å€¼: {rope_weights.mean():.6f}")
    
    # æ¯”è¾ƒè¾“å‡ºçš„å·®å¼‚
    output_diff = torch.mean(torch.abs(traditional_output - rope_output))
    print(f"è¾“å‡ºå·®å¼‚: {output_diff:.6f}")
    
    # åˆ†ææ³¨æ„åŠ›æ¨¡å¼
    print("\næ³¨æ„åŠ›æ¨¡å¼åˆ†æ:")
    print(f"ä¼ ç»Ÿæ³¨æ„åŠ›æƒé‡æ–¹å·®: {traditional_weights.var():.6f}")
    print(f"RoPEæ³¨æ„åŠ›æƒé‡æ–¹å·®: {rope_weights.var():.6f}")


def rope_extrapolation_test():
    """RoPEå¤–æ¨èƒ½åŠ›æµ‹è¯•"""
    print("\n" + "=" * 50)
    print("RoPEå¤–æ¨èƒ½åŠ›æµ‹è¯•")
    print("=" * 50)
    
    d_model = 64
    max_train_len = 50
    test_len = 100  # è¶…å‡ºè®­ç»ƒé•¿åº¦
    
    # åˆ›å»ºRoPEå®ä¾‹
    rope = RoPE(d_model, max_seq_len=max_train_len)
    
    print(f"è®­ç»ƒæœ€å¤§é•¿åº¦: {max_train_len}")
    print(f"æµ‹è¯•é•¿åº¦: {test_len}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    q_test = torch.randn(1, 1, test_len, d_model)
    k_test = torch.randn(1, 1, test_len, d_model)
    
    # æµ‹è¯•å¤–æ¨èƒ½åŠ›
    try:
        q_rope, k_rope = rope(q_test, k_test, seq_len=test_len)
        print("âœ… RoPEæˆåŠŸå¤„ç†è¶…å‡ºè®­ç»ƒé•¿åº¦çš„åºåˆ—")
        print(f"å¤–æ¨åå¼ é‡å½¢çŠ¶: {q_rope.shape}")
        
        # éªŒè¯ä½ç½®ç¼–ç çš„è¿ç»­æ€§
        # è®¡ç®—ç›¸é‚»ä½ç½®çš„ç›¸ä¼¼åº¦
        similarities = []
        for i in range(test_len - 1):
            sim = torch.cosine_similarity(q_rope[0, 0, i], q_rope[0, 0, i+1], dim=0)
            similarities.append(sim.item())
        
        similarities = np.array(similarities)
        print(f"ç›¸é‚»ä½ç½®å¹³å‡ç›¸ä¼¼åº¦: {similarities.mean():.4f}")
        print(f"ç›¸ä¼¼åº¦æ ‡å‡†å·®: {similarities.std():.4f}")
        
    except Exception as e:
        print(f"âŒ RoPEå¤–æ¨å¤±è´¥: {e}")


def visualize_rope_patterns():
    """å¯è§†åŒ–RoPEçš„ä½ç½®ç¼–ç æ¨¡å¼"""
    print("\n" + "=" * 50)
    print("RoPEä½ç½®ç¼–ç æ¨¡å¼å¯è§†åŒ–")
    print("=" * 50)
    
    d_model = 64
    seq_len = 50
    
    # åˆ›å»ºRoPE
    rope = RoPE(d_model, max_seq_len=seq_len)
    
    # è·å–ä¸åŒä½ç½®çš„ç¼–ç 
    positions = torch.arange(seq_len).unsqueeze(0).unsqueeze(0).unsqueeze(-1)  # [1, 1, seq_len, 1]
    dummy_tensor = torch.ones(1, 1, seq_len, d_model)
    
    # åº”ç”¨ä½ç½®ç¼–ç 
    encoded = rope.apply_rotary_pos_emb(dummy_tensor)
    
    # è®¡ç®—ä½ç½®é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
    encoded_2d = encoded.squeeze().squeeze()  # [seq_len, d_model]
    similarity_matrix = torch.mm(encoded_2d, encoded_2d.t())
    
    print(f"ä½ç½®ç¼–ç çŸ©é˜µå½¢çŠ¶: {encoded_2d.shape}")
    print(f"ä½ç½®ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {similarity_matrix.shape}")
    
    # åˆ†æå¯¹è§’çº¿æ¨¡å¼ï¼ˆç›¸å¯¹ä½ç½®æ¨¡å¼ï¼‰
    diagonals = []
    for offset in range(-10, 11):
        if offset == 0:
            continue
        diagonal = torch.diagonal(similarity_matrix, offset=offset)
        diagonals.append(diagonal.mean().item())
    
    print(f"ä¸åŒç›¸å¯¹ä½ç½®çš„å¹³å‡ç›¸ä¼¼åº¦èŒƒå›´: {min(diagonals):.4f} - {max(diagonals):.4f}")
    
    # å°è¯•ä¿å­˜å¯è§†åŒ–ï¼ˆå¦‚æœmatplotlibå¯ç”¨ï¼‰
    try:
        plt.figure(figsize=(10, 8))
        plt.imshow(similarity_matrix.detach().numpy(), cmap='viridis')
        plt.title('RoPEä½ç½®ç¼–ç ç›¸ä¼¼åº¦çŸ©é˜µ')
        plt.xlabel('ä½ç½®')
        plt.ylabel('ä½ç½®')
        plt.colorbar()
        plt.savefig('rope_similarity_matrix.png', dpi=150, bbox_inches='tight')
        print("âœ… ç›¸ä¼¼åº¦çŸ©é˜µå·²ä¿å­˜ä¸º rope_similarity_matrix.png")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•ä¿å­˜å¯è§†åŒ–å›¾ç‰‡: {e}")


def rope_vs_sinusoidal_comparison():
    """RoPE vs æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç æ¯”è¾ƒ"""
    print("\n" + "=" * 50)
    print("RoPE vs æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç æ¯”è¾ƒ")
    print("=" * 50)
    
    d_model = 128
    seq_len = 64
    batch_size = 2
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, d_model)
    
    # åˆ›å»ºä½ç½®ç¼–ç å®ä¾‹
    sinusoidal_pe = PositionalEncoding(d_model, max_seq_len=seq_len)
    rope = RoPE(d_model, max_seq_len=seq_len)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # åº”ç”¨æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç 
    x_sin = x.transpose(0, 1)  # [seq_len, batch_size, d_model]
    x_sin_encoded = sinusoidal_pe(x_sin)
    x_sin_encoded = x_sin_encoded.transpose(0, 1)  # è½¬å› [batch_size, seq_len, d_model]
    
    # åº”ç”¨RoPEï¼ˆæ³¨æ„RoPEé€šå¸¸åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­åº”ç”¨ï¼‰
    # è¿™é‡Œæˆ‘ä»¬ç›´æ¥åº”ç”¨åˆ°è¾“å…¥ä¸Šä½œä¸ºæ¼”ç¤º
    x_rope_encoded = rope.apply_rotary_pos_emb(x)
    
    print(f"æ­£å¼¦ä½™å¼¦ç¼–ç åå½¢çŠ¶: {x_sin_encoded.shape}")
    print(f"RoPEç¼–ç åå½¢çŠ¶: {x_rope_encoded.shape}")
    
    # æ¯”è¾ƒç¼–ç æ•ˆæœ
    # è®¡ç®—æ¯ç§ç¼–ç æ–¹å¼å¯¹åŸå§‹ä¿¡å·çš„ä¿æŒç¨‹åº¦
    sin_preservation = torch.cosine_similarity(x.flatten(), x_sin_encoded.flatten(), dim=0)
    rope_preservation = torch.cosine_similarity(x.flatten(), x_rope_encoded.flatten(), dim=0)
    
    print(f"æ­£å¼¦ä½™å¼¦ç¼–ç åŸå§‹ä¿¡å·ä¿æŒåº¦: {sin_preservation:.4f}")
    print(f"RoPEç¼–ç åŸå§‹ä¿¡å·ä¿æŒåº¦: {rope_preservation:.4f}")
    
    # åˆ†æä½ç½®æ•æ„Ÿæ€§
    # äº¤æ¢ä¸¤ä¸ªä½ç½®ï¼Œçœ‹ç¼–ç çš„å˜åŒ–
    x_swapped = x.clone()
    x_swapped[:, [0, seq_len//2]] = x_swapped[:, [seq_len//2, 0]]
    
    x_sin_swapped = x_swapped.transpose(0, 1)
    x_sin_swapped = sinusoidal_pe(x_sin_swapped).transpose(0, 1)
    x_rope_swapped = rope.apply_rotary_pos_emb(x_swapped)
    
    sin_sensitivity = torch.norm(x_sin_encoded - x_sin_swapped) / torch.norm(x_sin_encoded)
    rope_sensitivity = torch.norm(x_rope_encoded - x_rope_swapped) / torch.norm(x_rope_encoded)
    
    print(f"æ­£å¼¦ä½™å¼¦ç¼–ç ä½ç½®æ•æ„Ÿæ€§: {sin_sensitivity:.4f}")
    print(f"RoPEç¼–ç ä½ç½®æ•æ„Ÿæ€§: {rope_sensitivity:.4f}")


def performance_comparison():
    """æ€§èƒ½æ¯”è¾ƒ"""
    print("\n" + "=" * 50)
    print("æ€§èƒ½æ¯”è¾ƒ")
    print("=" * 50)
    
    import time
    
    d_model = 512
    num_heads = 8
    seq_len = 128
    batch_size = 32
    num_iterations = 100
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, d_model)
    mask = create_padding_mask(torch.ones(batch_size, seq_len), pad_idx=0)
    
    # åˆ›å»ºæ¨¡å‹
    traditional_attn = MultiHeadAttention(d_model, num_heads)
    rope_attn = RoPEMultiHeadAttention(d_model, num_heads)
    
    # é¢„çƒ­
    for _ in range(10):
        with torch.no_grad():
            _ = traditional_attn(x, x, x, mask)
            _ = rope_attn(x, x, x, mask)
    
    # æµ‹è¯•ä¼ ç»Ÿæ³¨æ„åŠ›
    start_time = time.time()
    for _ in range(num_iterations):
        with torch.no_grad():
            _ = traditional_attn(x, x, x, mask)
    traditional_time = time.time() - start_time
    
    # æµ‹è¯•RoPEæ³¨æ„åŠ›
    start_time = time.time()
    for _ in range(num_iterations):
        with torch.no_grad():
            _ = rope_attn(x, x, x, mask)
    rope_time = time.time() - start_time
    
    print(f"ä¼ ç»Ÿæ³¨æ„åŠ›æ—¶é—´: {traditional_time:.4f}s ({num_iterations} æ¬¡è¿­ä»£)")
    print(f"RoPEæ³¨æ„åŠ›æ—¶é—´: {rope_time:.4f}s ({num_iterations} æ¬¡è¿­ä»£)")
    print(f"æ€§èƒ½æ¯”ç‡ (RoPE/ä¼ ç»Ÿ): {rope_time/traditional_time:.2f}x")
    
    # è®¡ç®—å‚æ•°é‡
    traditional_params = sum(p.numel() for p in traditional_attn.parameters())
    rope_params = sum(p.numel() for p in rope_attn.parameters())
    
    print(f"ä¼ ç»Ÿæ³¨æ„åŠ›å‚æ•°é‡: {traditional_params:,}")
    print(f"RoPEæ³¨æ„åŠ›å‚æ•°é‡: {rope_params:,}")
    print(f"å‚æ•°é‡å·®å¼‚: {rope_params - traditional_params:,}")


def main():
    """ä¸»å‡½æ•°"""
    print("RoPE (Rotary Position Embedding) æ¼”ç¤º")
    print("å±•ç¤ºRoPEçš„ä¼˜åŠ¿å’Œä½¿ç”¨æ–¹æ³•")
    
    # 1. åŸºæœ¬ä½¿ç”¨
    basic_rope_example()
    
    # 2. æ³¨æ„åŠ›æ¯”è¾ƒ
    rope_attention_comparison()
    
    # 3. å¤–æ¨èƒ½åŠ›æµ‹è¯•
    rope_extrapolation_test()
    
    # 4. ä½ç½®ç¼–ç æ¯”è¾ƒ
    rope_vs_sinusoidal_comparison()
    
    # 5. å¯è§†åŒ–æ¨¡å¼
    visualize_rope_patterns()
    
    # 6. æ€§èƒ½æ¯”è¾ƒ
    performance_comparison()
    
    print("\n" + "=" * 50)
    print("RoPEæ¼”ç¤ºå®Œæˆ!")
    print("=" * 50)
    
    print("\nğŸ“ æ€»ç»“:")
    print("âœ… RoPEçš„ä¼˜åŠ¿:")
    print("  - æ›´å¥½çš„å¤–æ¨èƒ½åŠ›ï¼ˆå¯ä»¥å¤„ç†æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—ï¼‰")
    print("  - ç›´æ¥ç¼–ç ç›¸å¯¹ä½ç½®ä¿¡æ¯")
    print("  - ä¸éœ€è¦é¢å¤–çš„ä½ç½®åµŒå…¥å‚æ•°")
    print("  - åœ¨é•¿åºåˆ—ä¸Šè¡¨ç°æ›´å¥½")
    print("\nğŸ”§ ä½¿ç”¨å»ºè®®:")
    print("  - é€‚åˆéœ€è¦å¤„ç†å¯å˜é•¿åº¦åºåˆ—çš„ä»»åŠ¡")
    print("  - åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚")
    print("  - å¯ä»¥ä¸ä¼ ç»Ÿä½ç½®ç¼–ç æ··åˆä½¿ç”¨")


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥è·å¾—å¯é‡ç°çš„ç»“æœ
    torch.manual_seed(42)
    np.random.seed(42)
    
    main() 